{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W207 Final Project\n",
    "# Data Processing\n",
    "\n",
    "## 7 August 2018\n",
    "\n",
    "### Introduction  \n",
    "The data for this project is sourced through various users using the OpenBarbell device and collected by gym owner Jordan Berke. As granted by the owner, no NDA is required and we are free to work with the data as we please. The data is stored as json and the schema is openly shared by OpenBarbell through their [github wiki page](https://github.com/squatsandsciencelabs/OpenBarbell-V3/wiki/OpenBarbell-Data-Storage-Format).\n",
    "\n",
    "The raw data contains unclean data, data from outdated devices, and nested data structures that need to be unpacked. Due to the complexity of the data processing required, this data pipeline is held within its own notebook. At the end of the pipeline, a light-weight and clean csv file will be produces to serve as the base for all future operations.\n",
    "\n",
    "# 1. Import the Raw Data  \n",
    "This can take 5-10 minutes as the raw json file is quite large. This import also showcases the need for a simpler, cleaner data file to work off of. It is simply not productive to wait this long for whenever we need to iterate through the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.read_json('../obdatadump.json', lines = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Processing and Cleaning  \n",
    "The major issues to address within our pipeline:  \n",
    "- **Invalid Data Rows**\n",
    "    - There are various flags within the data that explicitly indicate when a row is invalid. Additionally, we have established logical thresholds for particular data fields so that we can reliably weed out rows with extremely erroneous data \n",
    "- **Cleaning User Input**\n",
    "    - Various fields are provided through user input. As these fields are optional, freeform, and unstandardized, there can be a lot of issues with the data. It is especially troubling since these fields represent our labels and important features.\n",
    "- **Unnesting Inner Data Structures**\n",
    "    - The raw data is hierarchical, with repetition data for a set being stored as a list in each set row. Furthermore, within the repetition data is another list for various data fields. All of this information needs to be unpacked to reveal all the various datapoints available in the data.\n",
    "- **Joining the Data Together**\n",
    "    - Following up on unnesting the data, we then need to join all of that data back together. The goal is to have a single row per repetition, containing columns for its parent set, the repetition itself, and repetition detail.\n",
    "- **Re-Index the Data**\n",
    "    - Unique identifiers are already provided for sets and repetitions. However, they are overly complex and are tightly coupled with the raw data. Simple integer IDs refactored to the transformed data are more useful for our project.\n",
    "\n",
    "\n",
    "## 2.1 Processing Set Data  \n",
    "Please refer to the inline comments for more information about this step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127832 rows in the raw set data.\n",
      "Pruning invalid rows...\n",
      "Coercing datetime fields..\n",
      "Fixing rpe values...\n",
      "Fixing weight values...\n",
      "Set data processing complete! 53115 rows after processing.\n"
     ]
    }
   ],
   "source": [
    "raw_set_ct = df_raw.shape[0]\n",
    "print(f\"{raw_set_ct} rows in the raw set data.\")\n",
    "\n",
    "# Remove rows that are invalid. This includes rows flagged as\n",
    "# removed, flagged as deleted, or have null exercise values\n",
    "print(\"Pruning invalid rows...\")\n",
    "df_set = df_raw.loc[df_raw['removed'] == 0]\n",
    "df_set = df_set.loc[(df_set['deleted'].isnull()) | (df_set['deleted'] == 0)]\n",
    "df_set = df_set.loc[df_set['exercise'].notnull()]\n",
    "\n",
    "# Create clean exercise labels for our data. A label is determined as 'clean'\n",
    "# if it matches one of the qualifying strings. This helps remove variations\n",
    "# of the big 3 lifts. We then remove any rows that we are not able to cleanly\n",
    "# label\n",
    "clean_bench = ['bench', 'bench press', 'bp', 'competition bench', 'comp bench']\n",
    "clean_squat = ['squat', 'back squat', 'competition squat', 'comp squat']\n",
    "clean_deadlift = [\"deadlift\", \"sumo deadlift\", \"conventional deadlift\", \"comp deadlift\", \"competition deadlift\"]\n",
    "def clean_exercise(e):\n",
    "    e = e.lower().strip()\n",
    "    if e in clean_bench:\n",
    "        return 'bench'\n",
    "    if e in clean_squat:\n",
    "        return 'squat'\n",
    "    if e in clean_deadlift:\n",
    "        return 'deadlift'\n",
    "    return 'other'\n",
    "    \n",
    "df_set['exercise_clean'] = df_set['exercise'].apply(clean_exercise)\n",
    "df_set = df_set.loc[df_set['exercise_clean'] != 'other']\n",
    "\n",
    "# Coerce datetime fields into their correct type\n",
    "print(\"Coercing datetime fields..\")\n",
    "date_cols = ['endTime', 'startTime','initialStartTime']\n",
    "df_set[date_cols] = df_set[date_cols].apply(pd.to_datetime, errors = 'coerce')\n",
    "\n",
    "# Rate of perceived exertion (rpe) is entered through user input. This \n",
    "# requires some extra effort on our end to clean the data. This includes \n",
    "# delimitting, parsing, and capping values.\n",
    "print(\"Fixing rpe values...\")\n",
    "def fix_rpe(r):\n",
    "    rvals = []\n",
    "    if type(r) == str:\n",
    "        for rval in r.split('-'):\n",
    "            try:\n",
    "                rvals.append(np.minimum(float(rval.replace(',', '.')), 10))\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    if type(r) in (float, int):\n",
    "        rvals.append(np.minimum(r, 10))\n",
    "    \n",
    "    return np.mean(rvals) if len(rvals) > 0 else np.nan\n",
    "\n",
    "df_set['rpe_num'] = df_set['rpe'].apply(fix_rpe)\n",
    "                             \n",
    "# Weight is also entered through user input and needs some\n",
    "# special processing. We account for european notation, delimited\n",
    "# lists, and cap outliers at 1000 lbs.\n",
    "print(\"Fixing weight values...\")\n",
    "def fix_weight(row, metric_col):\n",
    "    wvals = []\n",
    "    conversion_factor = 1 if row[metric_col] == 'lbs' else 2.20462\n",
    "    w = row['weight']\n",
    "    \n",
    "    if type(w) == str:\n",
    "        w = w.replace(',','.') if w.count(',') == 1 else w\n",
    "        dlm = '-' if w.count('-') > 0 else ','\n",
    "        dlm = '.' if w.count('.') > 1 else dlm\n",
    "        \n",
    "        for wval in w.split(dlm):\n",
    "            if dlm == '.' and (wval == '' or wval == '5'):\n",
    "                continue\n",
    "                             \n",
    "            try:\n",
    "                wvals.append(np.minimum(float(wval) * conversion_factor, 1000))\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    if type(w) in ('int', 'float'):\n",
    "        wvals.append(np.minimum(w * conversion_factor, 1000))\n",
    "    \n",
    "    return np.mean(wvals) if len(wvals) > 0 else np.nan\n",
    "\n",
    "df_set['weight_lbs'] = df_set.apply(fix_weight, axis=1, metric_col='metric')\n",
    "df_set = df_set.loc[df_set['weight_lbs'] > 0]\n",
    "\n",
    "print(f\"Set data processing complete! {df_set.shape[0]} rows after processing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Processing Rep Data  \n",
    "Please refer to the inline comments for more information about this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing out reps...\n",
      "Aggregating rep data...\n",
      "211277 rows in the raw rep data.\n",
      "Parsing rep data fields...\n",
      "Pruning invalid rows..\n",
      "Coercing numeric values..\n",
      "Rep processing complete! 181224 rows after processing.\n"
     ]
    }
   ],
   "source": [
    "# Pull setID and reps data into its own dataframe\n",
    "df_rep = df_set[['setID','reps']]\n",
    "\n",
    "# Reps are nested within a dict for each set row. For our purposes, we\n",
    "# want to explode out each rep into an individual row. To do that we\n",
    "# apply a dict parser to pull out the reps and concatenate them back\n",
    "# together into a dataframe\n",
    "print(\"Parsing out reps...\")\n",
    "dfs = []\n",
    "def dict_to_df(row, dict_col):\n",
    "    dict_df = pd.DataFrame.from_dict(row['reps'])\n",
    "    dfs.append(dict_df.assign(**row.drop(dict_col)))\n",
    "\n",
    "df_rep.apply(dict_to_df, axis=1, dict_col='reps')\n",
    "print(\"Aggregating rep data...\")\n",
    "df_rep = pd.concat(dfs)\n",
    "raw_rep_ct = df_rep.shape[0]\n",
    "print(f\"{raw_rep_ct} rows in the raw rep data.\")\n",
    "\n",
    "# There is are additional nested data fields within the rep data that\n",
    "# needs to be parsed out into their own columns. After parsing and\n",
    "# appending these data fields, drop the original raw rep data\n",
    "print(\"Parsing rep data fields...\")\n",
    "colnames = ['StartMessg', 'RepN', 'AvgVel', 'ROM', 'PeakVel', 'PeakVelLoc', 'PeakAccel', 'RepDur'\n",
    "            , 'TimeBWReps', 'TimeRepComp', 'TimeRepWait', 'SlowAllow', 'Backlight','MinAllow']\n",
    "df_rep[colnames] = pd.DataFrame([row[:14] for row in df_rep['data'].values.tolist()], index=df_rep.index)\n",
    "df_rep.drop(columns=['data'], inplace=True)\n",
    "\n",
    "# Remove invalid rows. This includes reps not flagged as 'isValid', flagged\n",
    "# as 'removed'.\n",
    "print(\"Pruning invalid rows..\") #219626\n",
    "df_rep = df_rep.loc[(df_rep['isValid'] == True) & (df_rep['removed'] == False)]\n",
    "\n",
    "# Through meta-analysis of the file, we have determined that >86% of our data\n",
    "# uses OpenBarbellv3. We felt this was sufficient to ensure good modeling. \n",
    "# Since there are numerous differences between each major version, standardizing \n",
    "# our data to just a single version also helps make life easier.\n",
    "df_rep = df_rep.loc[df_rep['appVersion'].apply(lambda x: str(x)[0] == '3')]\n",
    "\n",
    "# Coerce the numeric fields into their correct type\n",
    "print(\"Coercing numeric values..\") #219626\n",
    "num_cols = ['RepN', 'AvgVel', 'ROM', 'PeakVel', 'PeakVelLoc', 'PeakAccel', 'RepDur', 'TimeBWReps'\n",
    "            , 'TimeRepComp', 'TimeRepWait', 'SlowAllow', 'MinAllow']\n",
    "df_rep[num_cols] = df_rep[num_cols].apply(pd.to_numeric, errors = 'coerce')\n",
    "    \n",
    "# Some of the velocity fields have 'infinity' values. Replace them with nans\n",
    "df_rep = df_rep.replace(np.inf, np.nan)\n",
    "\n",
    "# After correcting the numeric fields, we can perform additional validity checks.\n",
    "# Primarily checking if the numeric fields are within an acceptable range.\n",
    "df_rep = df_rep.loc[(df_rep['AvgVel'] > 0) & (df_rep['AvgVel'] <= 3)]\n",
    "df_rep = df_rep.loc[df_rep['ROM'] <= 2000]\n",
    "df_rep = df_rep.loc[(df_rep['PeakAccel'] <= 3000) | (df_rep['PeakVel'] <= 10)]\n",
    "df_rep = df_rep.loc[(df_rep['PeakVelLoc'] > 0) & (df_rep['PeakVelLoc'] <= 100)]\n",
    "\n",
    "print(f\"Rep processing complete! {df_rep.shape[0]} rows after processing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Combining Set and Rep Data  \n",
    "At this step, both our set and rep dataframes have completed processing and cleaning. We can now merge the two by joining on the 'setID'. We apply an inner join, requiring a match between both dataframes. Any columns that appear in both dataframes and given a suffix to help identify its original source.\n",
    "\n",
    "After joining, we essentially have all of our data finalized. It is at this point we can re-index our rows to make future processing easier. We factorize the setID across the entire dataframe and factorize RepN within each set grouping. To follow standard indexing practice, we start the indices at 1.\n",
    "\n",
    "Next, we check to see just how much of the original data was pruned from the beginning of our pipeline to the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining set and rep data...\n",
      "Full data processing complete! 181224 rows after processing.\n",
      "Percent of data rows dropped after cleaning: 14.22%\n"
     ]
    }
   ],
   "source": [
    "print(\"Combining set and rep data...\")\n",
    "df_full = df_set.merge(df_rep, on =\"setID\", suffixes=('_set', '_rep'))\n",
    "\n",
    "df_full['setID_ix'] = df_full['setID'].factorize()[0] + 1\n",
    "df_full['RepCount'] = df_full.groupby('setID_ix')['RepN'].transform(lambda x: pd.factorize(x)[0] + 1)\n",
    "\n",
    "print(f\"Full data processing complete! {df_full.shape[0]} rows after processing.\")\n",
    "dropped = 1 - (df_full.shape[0]/ raw_rep_ct)\n",
    "print(\"Percent of data rows dropped after cleaning: {:.2%}\".format(dropped))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Export to CSV  \n",
    "Our final step is to simply write out our final dataframe to a csv file in our working directory. This csv file will be referenced for the remaining sections of our project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting to final csv...\n"
     ]
    }
   ],
   "source": [
    "print(\"Exporting to final csv...\")\n",
    "df_full.to_csv('./ob_data_w207_filtered.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
